#!/usr/bin/env python3
"""
CS1Profiler CSV Analysis Tool
Cities: Skylines 1 ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’è§£æãƒ»å¯è¦–åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import argparse
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆWindowsç’°å¢ƒå¯¾å¿œï¼‰
plt.rcParams['font.family'] = ['DejaVu Sans', 'Yu Gothic', 'Hiragino Sans', 'Noto Sans CJK JP']
plt.rcParams['figure.figsize'] = (12, 8)

class CS1ProfilerAnalyzer:
    def __init__(self, csv_file):
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§åˆæœŸåŒ–"""
        self.csv_file = csv_file
        self.df = None
        self.load_data()
    
    def load_data(self):
        """CSVãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            self.df = pd.read_csv(self.csv_file)
            self.df['DateTime'] = pd.to_datetime(self.df['DateTime'])
            self.df['TotalDurationPerFrame'] = self.df['Duration(ms)'] * self.df['Count']
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(self.df)} ãƒ¬ã‚³ãƒ¼ãƒ‰")
            print(f"ğŸ“… æœŸé–“: {self.df['DateTime'].min()} ï½ {self.df['DateTime'].max()}")
            print(f"ğŸ® ãƒ•ãƒ¬ãƒ¼ãƒ ç¯„å›²: {self.df['FrameCount'].min()} ï½ {self.df['FrameCount'].max()}")
        except Exception as e:
            print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def method_statistics(self, spike_multiplier=2.0):
        """ãƒ¡ã‚½ãƒƒãƒ‰åˆ¥çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        print("\nğŸ“Š ãƒ¡ã‚½ãƒƒãƒ‰åˆ¥çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆä¸­...")
        
        method_stats = []
        
        for method_name, method_data in self.df.groupby('Description'):
            durations = method_data['Duration(ms)']
            frame_totals = method_data.groupby('FrameCount')['TotalDurationPerFrame'].sum()
            frame_calls = method_data.groupby('FrameCount')['Count'].sum()
            
            avg_duration = durations.mean()
            spike_threshold = avg_duration * spike_multiplier
            spike_records = method_data[method_data['Duration(ms)'] > spike_threshold]
            
            stats = {
                'MethodName': method_name,
                'Category': method_data['Category'].iloc[0],
                'TotalCalls': method_data['Count'].sum(),
                'AvgDurationMs': avg_duration,
                'MaxDurationMs': durations.max(),
                'MinDurationMs': durations.min(),
                'StdDevMs': durations.std(),
                'FramesActive': len(frame_totals),
                'AvgTotalPerFrameMs': frame_totals.mean(),
                'MaxTotalPerFrameMs': frame_totals.max(),
                'SpikeCount': len(spike_records),
                'SpikeThreshold': spike_threshold,
                'AvgCallsPerFrame': frame_calls.mean(),
                'MaxCallsPerFrame': frame_calls.max(),
                'MinCallsPerFrame': frame_calls.min(),
                'AvgMemoryMB': method_data['MemoryMB'].mean(),
                'MaxMemoryMB': method_data['MemoryMB'].max(),
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
                'TotalImpactMs': frame_totals.sum(),
                'ImpactPercentage': 0,  # å¾Œã§è¨ˆç®—
                'PerformanceScore': avg_duration * method_data['Count'].sum()  # å½±éŸ¿åº¦ã‚¹ã‚³ã‚¢
            }
            method_stats.append(stats)
        
        # DataFrameã«å¤‰æ›
        stats_df = pd.DataFrame(method_stats)
        
        # å½±éŸ¿åº¦ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—
        total_impact = stats_df['TotalImpactMs'].sum()
        stats_df['ImpactPercentage'] = (stats_df['TotalImpactMs'] / total_impact * 100)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        stats_df = stats_df.sort_values('PerformanceScore', ascending=False)
        
        return stats_df

    def frame_statistics(self):
        """ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        print("\nğŸ“ˆ ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆä¸­...")
        
        frame_stats = []
        
        for frame_num, frame_data in self.df.groupby('FrameCount'):
            total_duration = frame_data['TotalDurationPerFrame'].sum()
            top_methods = frame_data.nlargest(5, 'TotalDurationPerFrame')[['Description', 'TotalDurationPerFrame']]
            
            stats = {
                'FrameNumber': frame_num,
                'FrameTime': frame_data['DateTime'].iloc[0],
                'TotalFrameMs': total_duration,
                'TotalCalls': frame_data['Count'].sum(),
                'UniqueMethodCount': len(frame_data),
                'TotalMemoryMB': frame_data['MemoryMB'].sum(),
                'TopMethod': top_methods.iloc[0]['Description'] if len(top_methods) > 0 else '',
                'TopMethodMs': top_methods.iloc[0]['TotalDurationPerFrame'] if len(top_methods) > 0 else 0
            }
            frame_stats.append(stats)
        
        return pd.DataFrame(frame_stats)

    def category_statistics(self):
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        print("\nğŸ·ï¸ ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆä¸­...")
        
        category_stats = []
        
        for category, category_data in self.df.groupby('Category'):
            durations = category_data['Duration(ms)']
            frame_totals = category_data.groupby('FrameCount')['TotalDurationPerFrame'].sum()
            
            stats = {
                'Category': category,
                'MethodCount': len(category_data['Description'].unique()),
                'TotalCalls': category_data['Count'].sum(),
                'AvgDurationMs': durations.mean(),
                'MaxDurationMs': durations.max(),
                'StdDevMs': durations.std(),
                'TotalImpactMs': frame_totals.sum(),
                'AvgImpactPerFrameMs': frame_totals.mean(),
                'AvgMemoryMB': category_data['MemoryMB'].mean()
            }
            category_stats.append(stats)
        
        stats_df = pd.DataFrame(category_stats)
        return stats_df.sort_values('TotalImpactMs', ascending=False)

    def detect_performance_issues(self, method_stats):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’æ¤œå‡º"""
        print("\nğŸš¨ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’æ¤œå‡ºä¸­...")
        
        issues = []
        
        # é«˜è² è·ãƒ¡ã‚½ãƒƒãƒ‰ã®æ¤œå‡º
        high_impact = method_stats[method_stats['ImpactPercentage'] > 5.0]
        for _, method in high_impact.iterrows():
            issues.append({
                'Type': 'é«˜è² è·ãƒ¡ã‚½ãƒƒãƒ‰',
                'Method': method['MethodName'],
                'Issue': f"å…¨ä½“ã® {method['ImpactPercentage']:.1f}% ã‚’å ã‚ã‚‹é«˜è² è·",
                'Value': f"{method['AvgTotalPerFrameMs']:.2f}ms/frame",
                'Severity': 'HIGH' if method['ImpactPercentage'] > 10 else 'MEDIUM'
            })
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯å¤šç™ºãƒ¡ã‚½ãƒƒãƒ‰ã®æ¤œå‡º
        spike_methods = method_stats[method_stats['SpikeCount'] > 10]
        for _, method in spike_methods.iterrows():
            issues.append({
                'Type': 'ã‚¹ãƒ‘ã‚¤ã‚¯å¤šç™º',
                'Method': method['MethodName'],
                'Issue': f"{method['SpikeCount']} å›ã®ã‚¹ãƒ‘ã‚¤ã‚¯ç™ºç”Ÿ",
                'Value': f"æœ€å¤§ {method['MaxDurationMs']:.2f}ms",
                'Severity': 'HIGH' if method['SpikeCount'] > 50 else 'MEDIUM'
            })
        
        # å‘¼ã³å‡ºã—å›æ•°ç•°å¸¸ã®æ¤œå‡º
        call_variance = method_stats[
            (method_stats['MaxCallsPerFrame'] / method_stats['AvgCallsPerFrame'] > 3) &
            (method_stats['AvgCallsPerFrame'] > 1)
        ]
        for _, method in call_variance.iterrows():
            issues.append({
                'Type': 'å‘¼ã³å‡ºã—å›æ•°å¤‰å‹•',
                'Method': method['MethodName'],
                'Issue': f"æœ€å¤§ {method['MaxCallsPerFrame']:.0f} å›/frame (å¹³å‡ {method['AvgCallsPerFrame']:.1f})",
                'Value': f"å¤‰å‹•ç‡ {method['MaxCallsPerFrame']/method['AvgCallsPerFrame']:.1f}x",
                'Severity': 'MEDIUM'
            })
        
        return pd.DataFrame(issues)

    def generate_visualizations(self, method_stats, frame_stats, output_dir='analysis_output'):
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        print(f"\nğŸ“Š å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆä¸­... ({output_dir}/)")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. ãƒˆãƒƒãƒ—15ãƒ¡ã‚½ãƒƒãƒ‰ã®å½±éŸ¿åº¦
        plt.figure(figsize=(14, 8))
        top15 = method_stats.head(15)
        bars = plt.barh(range(len(top15)), top15['AvgTotalPerFrameMs'])
        plt.yticks(range(len(top15)), [name[:40] + '...' if len(name) > 40 else name for name in top15['MethodName']])
        plt.xlabel('å¹³å‡å½±éŸ¿åº¦ (ms/frame)')
        plt.title('CS1Profiler: ãƒˆãƒƒãƒ—15 é«˜è² è·ãƒ¡ã‚½ãƒƒãƒ‰')
        plt.gca().invert_yaxis()
        
        # ãƒãƒ¼ã«æ•°å€¤ã‚’è¡¨ç¤º
        for i, bar in enumerate(bars):
            width = bar.get_width()
            plt.text(width + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{width:.2f}ms', ha='left', va='center')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/top15_methods.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥å½±éŸ¿åº¦ï¼ˆå††ã‚°ãƒ©ãƒ•ï¼‰
        category_stats = self.category_statistics()
        plt.figure(figsize=(10, 8))
        plt.pie(category_stats['TotalImpactMs'], labels=category_stats['Category'], autopct='%1.1f%%')
        plt.title('ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿åº¦')
        plt.savefig(f'{output_dir}/category_impact.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥è² è·æ¨ç§»
        plt.figure(figsize=(15, 6))
        plt.plot(frame_stats['FrameNumber'], frame_stats['TotalFrameMs'], alpha=0.7)
        plt.xlabel('ãƒ•ãƒ¬ãƒ¼ãƒ ç•ªå·')
        plt.ylabel('ç·å‡¦ç†æ™‚é–“ (ms)')
        plt.title('ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥å‡¦ç†æ™‚é–“æ¨ç§»')
        plt.grid(True, alpha=0.3)
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å¼·èª¿è¡¨ç¤º
        spike_threshold = frame_stats['TotalFrameMs'].mean() + frame_stats['TotalFrameMs'].std() * 2
        spikes = frame_stats[frame_stats['TotalFrameMs'] > spike_threshold]
        if len(spikes) > 0:
            plt.scatter(spikes['FrameNumber'], spikes['TotalFrameMs'], 
                       color='red', s=50, alpha=0.8, label=f'ã‚¹ãƒ‘ã‚¤ã‚¯({len(spikes)}å›)')
            plt.legend()
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/frame_timeline.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. ã‚¹ãƒ‘ã‚¤ã‚¯åˆ†æï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
        spike_methods = method_stats[method_stats['SpikeCount'] > 0].head(10)
        if len(spike_methods) > 0:
            plt.figure(figsize=(12, 6))
            plt.bar(range(len(spike_methods)), spike_methods['SpikeCount'])
            plt.xticks(range(len(spike_methods)), 
                      [name[:20] + '...' if len(name) > 20 else name for name in spike_methods['MethodName']], 
                      rotation=45, ha='right')
            plt.ylabel('ã‚¹ãƒ‘ã‚¤ã‚¯å›æ•°')
            plt.title('ãƒ¡ã‚½ãƒƒãƒ‰åˆ¥ã‚¹ãƒ‘ã‚¤ã‚¯ç™ºç”Ÿå›æ•° (Top10)')
            plt.tight_layout()
            plt.savefig(f'{output_dir}/spike_analysis.png', dpi=300, bbox_inches='tight')
            plt.close()

    def export_results(self, method_stats, frame_stats, issues, output_dir='analysis_output'):
        """è§£æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        print(f"\nğŸ’¾ è§£æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­... ({output_dir}/)")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # çµ±è¨ˆçµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        method_stats.to_csv(f'{output_dir}/method_statistics.csv', index=False, encoding='utf-8-sig')
        frame_stats.to_csv(f'{output_dir}/frame_statistics.csv', index=False, encoding='utf-8-sig')
        issues.to_csv(f'{output_dir}/performance_issues.csv', index=False, encoding='utf-8-sig')
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        with open(f'{output_dir}/analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write("CS1Profiler è§£æãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 50 + "\n")
            f.write(f"è§£ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {self.csv_file}\n")
            f.write(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(self.df)}\n")
            f.write(f"è§£æãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {self.df['FrameCount'].nunique()}\n")
            f.write(f"ç·ãƒ¡ã‚½ãƒƒãƒ‰æ•°: {self.df['Description'].nunique()}\n\n")
            
            # ãƒˆãƒƒãƒ—å•é¡Œ
            f.write("ğŸš¨ ä¸»è¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ\n")
            f.write("-" * 30 + "\n")
            for _, issue in issues.head(10).iterrows():
                f.write(f"[{issue['Severity']}] {issue['Type']}: {issue['Method']}\n")
                f.write(f"  è©³ç´°: {issue['Issue']} ({issue['Value']})\n\n")
            
            # ãƒˆãƒƒãƒ—ãƒ¡ã‚½ãƒƒãƒ‰
            f.write("ğŸ“Š é«˜è² è·ãƒ¡ã‚½ãƒƒãƒ‰ Top10\n")
            f.write("-" * 30 + "\n")
            for i, (_, method) in enumerate(method_stats.head(10).iterrows(), 1):
                f.write(f"{i:2d}. {method['MethodName']}\n")
                f.write(f"    å½±éŸ¿åº¦: {method['AvgTotalPerFrameMs']:.2f}ms/frame ({method['ImpactPercentage']:.1f}%)\n")
                f.write(f"    å‘¼ã³å‡ºã—: {method['TotalCalls']} å›, ã‚¹ãƒ‘ã‚¤ã‚¯: {method['SpikeCount']} å›\n\n")

    def run_full_analysis(self, output_dir='analysis_output'):
        """å®Œå…¨è§£æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ CS1Profiler å®Œå…¨è§£æã‚’é–‹å§‹...")
        
        # çµ±è¨ˆç”Ÿæˆ
        method_stats = self.method_statistics()
        frame_stats = self.frame_statistics()
        issues = self.detect_performance_issues(method_stats)
        
        # å¯è¦–åŒ–
        self.generate_visualizations(method_stats, frame_stats, output_dir)
        
        # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        self.export_results(method_stats, frame_stats, issues, output_dir)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print("\n" + "="*60)
        print("ğŸ¯ CS1Profiler è§£æçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        
        print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä¸Šä½5ãƒ¡ã‚½ãƒƒãƒ‰:")
        for i, (_, method) in enumerate(method_stats.head(5).iterrows(), 1):
            print(f"{i}. {method['MethodName'][:50]}")
            print(f"   {method['AvgTotalPerFrameMs']:.2f}ms/frame ({method['ImpactPercentage']:.1f}%)")
        
        print(f"\nğŸš¨ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(issues)} ä»¶")
        high_issues = issues[issues['Severity'] == 'HIGH']
        if len(high_issues) > 0:
            print("   é«˜é‡è¦åº¦å•é¡Œ:")
            for _, issue in high_issues.head(3).iterrows():
                print(f"   - {issue['Type']}: {issue['Method'][:40]}...")
        
        print(f"\nğŸ“ è©³ç´°çµæœ: {output_dir}/ ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        print("   - method_statistics.csv: ãƒ¡ã‚½ãƒƒãƒ‰åˆ¥çµ±è¨ˆ")
        print("   - frame_statistics.csv: ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥çµ±è¨ˆ") 
        print("   - performance_issues.csv: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ")
        print("   - analysis_report.txt: è§£æãƒ¬ãƒãƒ¼ãƒˆ")
        print("   - *.png: å¯è¦–åŒ–ã‚°ãƒ©ãƒ•")

def main():
    parser = argparse.ArgumentParser(description='CS1Profiler CSV Analysis Tool')
    parser.add_argument('csv_file', help='CS1Profilerã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('-o', '--output', default='analysis_output', help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: analysis_output)')
    parser.add_argument('-s', '--spike-multiplier', type=float, default=2.0, help='ã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡ºã®é–¾å€¤å€ç‡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2.0)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.csv_file):
        print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.csv_file}")
        return
    
    try:
        analyzer = CS1ProfilerAnalyzer(args.csv_file)
        analyzer.run_full_analysis(args.output)
        print("\nâœ… è§£æå®Œäº†!")
    except Exception as e:
        print(f"âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
        raise

if __name__ == '__main__':
    main()
